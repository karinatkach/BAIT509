•	Bagging is a special case of random forests under which case?
When number of variables we limit to when choosing the initial node of the tree is equal to the total number of variables existing in the dataset (to predict Y)
•	What are the hyperparameters we can control for random forests?
Number of datasets we create through bootstrapping + number of variables we limit to when choosing the initial node of the tree 
•	Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
i.	(1,0), (1,2), (1,5) – not valid; (1,0) observation is actually not present in the dataset we are given
ii.	(1,2), (2,0) –valid; both observations in the new dataset are present in the original dataset.
iii.	(1,2), (1,2), (1,5) –valid; all observations in the new dataset are present in the original dataset and repeating values are acceptable. 
•	For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?
ii. (1,2), (2,0) – the out of bag observation is (1,5)
iii. (1,2), (1,2), (1,5) – the out of bag observation is (2,0)
•	You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
i.	Regression: your trees make the following four predictions: 1,1,3,3 – 2 is our prediction (average)
ii.	Classification: your trees make the following four predictions: "A", "A", "B", "C" – “A” is our prediction (common vote)
